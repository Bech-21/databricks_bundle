-- Databricks notebook source
-- MAGIC %md
-- MAGIC ### Example Exploratory Notebook
-- MAGIC
-- MAGIC Use this notebook to explore the data generated by the pipeline in your preferred programming language.
-- MAGIC
-- MAGIC **Note**: This notebook is not executed as part of the pipeline.

-- COMMAND ----------

-- MAGIC %python
-- MAGIC import sys
-- MAGIC
-- MAGIC sys.path.append("/Workspace/Users/bachirmahomad@gmail.com/pl_earthquake_rig_data_dlt")

-- COMMAND ----------

-- !!! Before performing any data analysis, make sure to run the pipeline to materialize the sample datasets. The tables referenced in this notebook depend on that step.

USE CATALOG `earth_data`;
USE SCHEMA `lakehouse`;

SELECT * from sample_aggregation_pl_earthquake_rig_data_dlt;

-- COMMAND ----------

SELECT * from earth_data.lakehouse.silver_earthquake

-- COMMAND ----------

import dlt
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql import Window

def create_silver_earthquake_cleaned():
    """Clean and prepare earthquake data"""
    earthquake_df = spark.readStream.table("earth_data.lakehouse.silver_earthquake")
    
    return (
        earthquake_df
        .withColumn('longitude', when(col('longitude').isNull(), lit(0.0)).otherwise(col('longitude')))
        .withColumn('latitude', when(col('latitude').isNull(), lit(0.0)).otherwise(col('latitude')))
        .withColumn('time', when(col('time').isNull(), lit(0)).otherwise(col('time')))
        .withColumn('event_time', (col('time') / 1000).cast(TimestampType()))
        .withColumn('updated_time', (col('updated') / 1000).cast(TimestampType()))
)
    



def create_earthquake_with_location():
    """Add county information using spatial join"""
    
    # Read cleaned earthquake data (batch mode for join)
    earthquakes = dlt.read("silver_earthquake_cleaned")
    
    # Read US counties reference data
    counties = spark.read.table("earth_data.lakehouse.us_counties").select(
        col('county_name'),
        col('state_name_Short').alias('state_abbr'),
        col('state_name_long').alias('state_name'),
        col('latitude').alias('county_lat'),
        col('longitude').alias('county_lon')
    )
    
    # Calculate approximate distance using simplified formula
    # For US coordinates, this is good enough
    earthquakes_with_counties = earthquakes.crossJoin(broadcast(counties))
    
    earthquakes_with_counties = earthquakes_with_counties.withColumn(
        'distance',
        # Simplified distance in degrees (multiply by ~69 miles or ~111 km)
        sqrt(
            pow(col('latitude') - col('county_lat'), 2) +
            pow(col('longitude') - col('county_lon'), 2)
        )
    )
    
    # Find the nearest county for each earthquake
    window_spec = Window.partitionBy('id').orderBy('distance')
    
    result = (
        earthquakes_with_counties
        .withColumn('rank', row_number().over(window_spec))
        .filter(col('rank') == 1)
        .select(
            col('id'),
            col('magnitude'),
            col('place'),
            col('event_time'),
            col('updated_time'),
            col('latitude'),
            col('longitude'),
            col('depth'),
            col('status'),
            col('tsunami'),
            col('county_name').alias('county'),
            col('state_abbr'),
            col('state_name')
        )
    )
    
    return result

-- COMMAND ----------

-- MAGIC %python
-- MAGIC
-- MAGIC
-- MAGIC def create_earthquake_with_location():
-- MAGIC     """Add county information using spatial join"""
-- MAGIC     
-- MAGIC     # Read cleaned earthquake data (batch mode for join)
-- MAGIC     earthquakes = spark.readStream.table("silver_earthquake_cleaned")
-- MAGIC     
-- MAGIC     # Read US counties reference data
-- MAGIC     counties = spark.read.table("/Volumes/earth_data/bronze/operationaldata/US_Counties_Centroids.csv").select(
-- MAGIC         col('NAME').alias('county'),
-- MAGIC         col('STATE').alias('state_name'),
-- MAGIC         col('LATITUDE').alias('county_lat'),
-- MAGIC         col('LONGITUDE').alias('county_lon')
-- MAGIC     )
-- MAGIC     
-- MAGIC     # Calculate approximate distance using simplified formula
-- MAGIC     # For US coordinates, this is good enough
-- MAGIC     earthquakes_with_counties = earthquakes.crossJoin(broadcast(counties))
-- MAGIC     
-- MAGIC     earthquakes_with_counties = earthquakes_with_counties.withColumn(
-- MAGIC         'distance',
-- MAGIC         # Simplified distance in degrees (multiply by ~69 miles or ~111 km)
-- MAGIC         sqrt(
-- MAGIC             pow(col('latitude') - col('county_lat'), 2) +
-- MAGIC             pow(col('longitude') - col('county_lon'), 2)
-- MAGIC         )
-- MAGIC     )
-- MAGIC     
-- MAGIC     # Find the nearest county for each earthquake
-- MAGIC     window_spec = Window.partitionBy('id').orderBy('distance')
-- MAGIC     
-- MAGIC     result = (
-- MAGIC         earthquakes_with_counties
-- MAGIC         .withColumn('rank', row_number().over(window_spec))
-- MAGIC         .filter(col('rank') == 1)
-- MAGIC         .select(
-- MAGIC             col('id'),
-- MAGIC             col('magnitude'),
-- MAGIC             col('place'),
-- MAGIC             col('event_time'),
-- MAGIC             col('updated_time'),
-- MAGIC             col('latitude'),
-- MAGIC             col('longitude'),
-- MAGIC             col('depth'),
-- MAGIC             col('status'),
-- MAGIC             col('tsunami'),
-- MAGIC             col('county_name').alias('county'),
-- MAGIC             col('state_abbr'),
-- MAGIC             col('state_name')
-- MAGIC         )
-- MAGIC     )
-- MAGIC     
-- MAGIC     return result

-- COMMAND ----------

-- MAGIC %python
-- MAGIC %pip install reverse_geocoder
-- MAGIC
-- MAGIC import reverse_geocoder as rg
-- MAGIC import pandas as pd
-- MAGIC from pyspark.sql.functions import *
-- MAGIC from pyspark.sql.types import *

-- COMMAND ----------

-- MAGIC %python
-- MAGIC
-- MAGIC import reverse_geocoder as rg
-- MAGIC import pandas as pd
-- MAGIC from pyspark.sql.functions import *
-- MAGIC from pyspark.sql.types import *

-- COMMAND ----------

-- MAGIC %python
-- MAGIC
-- MAGIC def get_county_batch(lat_series: pd.Series, lon_series: pd.Series) -> pd.Series:
-- MAGIC     coords = [(float(lat), float(lon)) 
-- MAGIC               for lat, lon in zip(lat_series, lon_series) 
-- MAGIC               if pd.notna(lat) and pd.notna(lon)]
-- MAGIC     
-- MAGIC     if not coords:
-- MAGIC         return pd.Series([None] * len(lat_series))
-- MAGIC     
-- MAGIC     results = rg.search(coords)
-- MAGIC     counties = [r.get('admin2', '').upper().replace(' COUNTY', '').strip() 
-- MAGIC                 for r in results]
-- MAGIC     
-- MAGIC     return pd.Series(counties)
-- MAGIC     

-- COMMAND ----------

-- MAGIC %python
-- MAGIC df = spark.read.table(
-- MAGIC   "earth_data.lakehouse.silver_earthquake"
-- MAGIC ).toPandas()

-- COMMAND ----------

-- MAGIC %python
-- MAGIC df['county'] = get_county_batch(df['latitude'], df['longitude'])
-- MAGIC        
-- MAGIC df.head()

-- COMMAND ----------


